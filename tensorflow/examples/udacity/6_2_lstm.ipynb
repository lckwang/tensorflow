{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Assignment 6 -- Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read embeddings, dictionary, text from pickle file, which was saved previously. \n",
    "\n",
    "**Note that text contains ascii charactors after reading.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organiza\n",
      "embedding size:\t 27\n",
      "final_embeddings:\t <class 'numpy.ndarray'>\n",
      "final_embeddings[32]:\tbf\n",
      " [ 0.2297595   0.11604198 -0.16186151  0.07841912 -0.27451089  0.01861068\n",
      " -0.20724232  0.0593353   0.2803089  -0.10459089 -0.2438134   0.29193261\n",
      " -0.36407951  0.16633166  0.13451004 -0.07307564  0.16797698  0.32035738\n",
      " -0.31689703  0.08712179  0.14797479 -0.05835203  0.13006845 -0.04253782\n",
      " -0.22647974 -0.07452152 -0.14715371]\n"
     ]
    }
   ],
   "source": [
    "f = open('TwoCharEmbedding.pckl', 'rb')\n",
    "embeddings, dictionary, reverse_dictionary, text = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "embedding_size = len(embeddings[0])\n",
    "\n",
    "aSlice=text[100:300]\n",
    "print(aSlice)\n",
    "print(\"embedding size:\\t\", embedding_size)\n",
    "print(\"final_embeddings:\\t\", type(embeddings))\n",
    "print('final_embeddings[32]:\\t%s\\n' % reverse_dictionary[32], embeddings[32, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99998999 ns anarchists advocate social relations based upon voluntary ass\n",
      "1000 anarchism originated as a term of abuse first used against early\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]  # The first 1000 chars are validation text\n",
    "train_text = text[valid_size:]  # The rest are training text\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' ' = 27\n",
    "\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):  # ID of a char is serail number of a lower case char, starting from 1. ID of ' ' is 0, 'a' is 1, 'b' is 2, etc.\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)   # Return ID 0 for non-space, non-lower-case ascii char\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid): # return ascii char based on char ID\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model. As this is a charactor prediction application, the training text is also the expected (label) text. The label text starts from the 2nd char while the training text starts from the first char.\n",
    "\n",
    "Note that batch, whose shape is (batch_size, 27), is not continuous in text order between batches. A char in each batch is taken from text in different locations.\n",
    "\n",
    "BatchGenerator::next() function returns (1+num_unrollings) batches, i.e. the shape of batches is (1+num_unrollings, batch_size, 27). Charactor in the same batch are in text sequence even across consecutive batches. This provides a continuous text within a batch for training. Learning (gradient descent) is executed every batches, i.e. every (1+num_unrollings, batch_size) charactors, i.e. each batch has (1+num_unrollings) continous text charactors.\n",
    "\n",
    "As an example, the first batch in three batches, each of which is returned by BatchGenerator::next(), has the following texts:\n",
    "'ons anarchi', 'ists advoca', 'ate social '\n",
    "Note that the above text are continous except that there is a duplicate char at the end of a string and beginning of its next string.\n",
    "\n",
    "batches2string() takes in batches parameter gernetated by BatchGenerator::next() and produces a list of batch_size strings . Each string in the returned list is restored to the text sequence of (1+num_unrollings) charactors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train batches: 12 64\n",
      "['ns anarchist', 'hen military', 'leria arches', 'abbeys and m', 'arried urrac', 'el and richa', ' and liturgi', 'y opened for', 'ion from the', 'igration too', 'ew york othe', 'e boeing sev', ' listed with', 'ber has prob', ' be made to ', 'er who recei', 're significa', ' fierce crit', 'two six eigh', 'ristotle s u', 'ty can be lo', 'and intracel', 'ion of the s', 'y to pass hi', ' certain dru', 't it will ta', ' convince th', 'nt told him ', 'mpaign and b', 'ver side sta', 'ous texts su', ' capitalize ', ' duplicate o', 'h ann es d h', 'ne january e', 'oss zero the', 'al theories ', 'st instance ', 'dimensional ', 'ost holy mor', ' s support o', ' is still di', ' oscillating', ' eight subty', 'f italy lang', ' the tower c', 'lahoma press', 'rprise linux', 's becomes th', 't in a nazi ', 'he fabian so', 'tchy to rela', 'sharman netw', 'sed emperor ', 'ing in polit', ' neo latin m', 'h risky risk', 'ncyclopedic ', 'ense the air', 'uating from ', 'reet grid ce', 'tions more t', 'ppeal of dev', 'i have made ']\n",
      "['sts advocate', 'ry governmen', 'es national ', ' monasteries', 'aca princess', 'hard baer h ', 'gical langua', 'or passenger', 'he national ', 'ook place du', 'her well kno', 'even six sev', 'th a gloss c', 'obably been ', 'o recognize ', 'eived the fi', 'cant than in', 'itic of the ', 'ght in signs', ' uncaused ca', 'lost as in d', 'ellular ice ', ' size of the', 'him a stick ', 'rugs confusi', 'take to comp', 'the priest o', 'm to name it', ' barred atte', 'tandard form', 'such as esot', 'e on the gro', ' of the orig', ' hiver one n', ' eight march', 'he lead char', 's classical ', 'e the non gm', 'l analysis f', 'ormons belie', ' or at least', 'disagreed up', 'ng system ex', 'types based ', 'nguages the ', ' commission ', 'ss one nine ', 'ux suse linu', 'the first da', 'i concentrat', 'society nehr', 'latively sti', 'tworks sharm', 'r hirohito t', 'itical initi', ' most of the', 'skerdoo rick', 'c overview o', 'ir component', 'm acnm accre', 'centerline e', ' than any ot', 'evotional bu', 'e such devic']\n",
      "\n",
      "shape of validation batches: 3 1\n",
      "['ana']\n",
      "['nar']\n",
      "['arc']\n",
      "['rch']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    # segment size, each batch data is taken from a segement\n",
    "    segment = self._text_size // batch_size  # text is split into batch_size(=64) segments\n",
    "    # index into text. distance between successive index are segment\n",
    "    self._cursor = np.array([ offset * segment for offset in range(batch_size)]) # use numpy array to make math on indexes easy\n",
    "    self._last_batch = self._next_batch()\n",
    "    self._pred_batch = self._next_batch()  # next char to _last_batch\n",
    "  \n",
    "  def _next_batch(self):  # A batch (whose shape is (batch_size)) is not continuous in text order.\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = [self._text[x] for x in self._cursor] # 'batch' shape is (batch_size).\n",
    "    self._cursor = (self._cursor + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):  # The function returns 1+num_unrollings batches.\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch, self._pred_batch]\n",
    "    for step in range(self._num_unrollings):  # Skip num_unrollings batches\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-2]\n",
    "    self._pred_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]  # np.argmax() returns the indices of the maximum values along an axis.\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation. The batches input is in one-hot encoding form. \"\"\"\n",
    "  #print(batches[0].shape)\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]  # This line restores text order in element of s\n",
    "  return s\n",
    "\n",
    "def batchesRestore(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation. The batches input is in text form. \"\"\"\n",
    "  s = [''] * len(batches[0])\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, b)]  # This line restores text order in element of s\n",
    "  return s\n",
    "\n",
    "def batch2data(batches):\n",
    "  \"\"\"Convert a sequence of batches into training data and labels.\n",
    "  The batches input is in text form. \"\"\"\n",
    "    \n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "batches = train_batches.next()\n",
    "print(\"shape of train batches:\", len(batches), len(batches[0]))\n",
    "print(batchesRestore(batches))\n",
    "print(batchesRestore(train_batches.next()))\n",
    "print()\n",
    "\n",
    "batches = valid_batches.next()\n",
    "print(\"shape of validation batches:\", len(batches), len(batches[0]))\n",
    "print(batchesRestore(batches))\n",
    "print(batchesRestore(valid_batches.next()))\n",
    "print(batchesRestore(valid_batches.next()))\n",
    "print(batchesRestore(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01608888  0.04855415  0.017047    0.02391935  0.05606703  0.04896242\n",
      "   0.02692701  0.02716298  0.00762555  0.09718637  0.01302754  0.00046574\n",
      "   0.03249313  0.00512914  0.00013004  0.01117684  0.04243033  0.05096304\n",
      "   0.01922752  0.05385489  0.02867424  0.09850775  0.08037328  0.06868541\n",
      "   0.03187602  0.01973218  0.07371217]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# logprob() measures the similarity between predictions and labels. It computes the cross entropy between them. \n",
    "# The formular is the same as loss calculation.\n",
    "# A single value is returned. The larger the returned value, the similarity is lower.\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  # np.log(): Natural logarithm, element-wise.\n",
    "  # np.sum(): Sum of array elements over a given axis.The default, axis=None, will sum all of the elements of the input array.\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  # random.uniform() returns a random floating point number N such that a <= N <= b for a <= b\n",
    "  r = random.uniform(0, 1) # return a random number between 0 and 1\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):  # Randomly produce a 1-hot encoding sample\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution(): # Generate a row of random probabilities\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  #print(np.sum(b, 1))\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "rd = random_distribution()\n",
    "print(rd)\n",
    "print(sample(rd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation. See Fig 10.6 in P373 and Fig 10.16 on P398 for this model.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    # tf.sigmoid() is an element wise sigmoid function. \n",
    "    # These gate's variables have a shape of (batch_size, num_nodes).\n",
    "    # (batch_size,vocabulary_size)* (vocabulary_size, num_nodes) = (batch_size, num_nodes)\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)  \n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    # update shape: (batch_size,vocabulary_size)* (vocabulary_size, num_nodes) = (batch_size, num_nodes)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    # state shape: (batch_size, num_nodes) * (batch_size, num_nodes) = (batch_size, num_nodes). \n",
    "    # Note this is an element wise multiplication.\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Note that train_inputs and train_labels have the same number of elements: num_unrollings\n",
    "  train_inputs = tf.placeholder(tf.float32, shape=[num_unrollings, batch_size, embedding_size]) \n",
    "  train_labels = tf.placeholder(tf.float32, shape=[num_unrollings, batch_size, vocabulary_size])\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output # Shape is (batch_size, num_nodes), initial value are all 0s.\n",
    "  state = saved_state   # Shape is (batch_size, num_nodes), initial value are all 0s.\n",
    "  for i in train_inputs:  # i is a (64, 27) matrix, i.e. (batch_size,vocabulary_size)\n",
    "    # output and state are (batch_size, num_nodes) matrix\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # outputs are output of num_nodes LSTM cells, with output of num_unrollings times. \n",
    "  # logits is calculated based on outputs which is calculated by the loop above.\n",
    "  # loss is caculated based on logits and train_labels, both of which contains data of a whole batch.\n",
    "  # gradient descent is performed after a whole batch is calculated.\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b) \n",
    "        # outputs is a list of num_unrollings elements each of which has shape(batch_size, num_nodes).\n",
    "        # tf.concat(outputs, 0) shape is (num_unrollings*batch_size, num_nodes) = (640, 32)\n",
    "        # Thus logits shape is (num_unrollings*batch_size, vocabulary_size) = (640, 27)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels,0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.5, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, embedding_size])\n",
    "  # The following two variables are not parametered with \"trainable=False\" becasue no gradient descent \n",
    "  # is performed on them.\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292642 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.91\n",
      "================================================================================\n",
      "qwfaari mftzo z faimpzqsziaqsozyzuddu ylae vr vt qq  lfpn podyol ecarvotnkgzhx g\n",
      "hiwzfbh  ej vn g ibeoqf  efpe wh gja  kwvitaedbxe euwhezbgodi ihodrazqkia onnuiv\n",
      "s b jftrfhcrfk nev yfatkufxhomloedd lbjviulrea ecedveiolatvgc qw a teouse oiy   \n",
      "rgf ayldsabawznluaecpbtdi nqit yrcfbkefn eilgvvb  semfjetmqcfyugeoevcbne qtsyhoa\n",
      "c    tpwea  sk ahefuh ktenes rqeciihipzikjzppavpaeopul ztrnngfretie x tsy ttl v \n",
      "\n",
      "Time taken to step 0: 0.28s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text:                     \n",
      "Validation set perplexity: 20.11\n",
      "\n",
      "Average loss at step 200: 2.435058 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.95\n",
      "Validation set perplexity: 9.22\n",
      "\n",
      "Average loss at step 400: 2.054268 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 7.77\n",
      "\n",
      "Average loss at step 600: 1.952981 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 7.11\n",
      "\n",
      "Average loss at step 800: 1.866939 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 7.04\n",
      "\n",
      "Average loss at step 1000: 1.805867 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.35\n",
      "\n",
      "Average loss at step 1200: 1.792439 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 5.84\n",
      "\n",
      "Average loss at step 1400: 1.774325 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.60\n",
      "\n",
      "Average loss at step 1600: 1.739700 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 5.44\n",
      "\n",
      "Average loss at step 1800: 1.733388 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.39\n",
      "\n",
      "Average loss at step 2000: 1.726033 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "================================================================================\n",
      "hing the fidst pic ficture aboliseed langely something maki pheanes that the pro\n",
      "is of howeing j ethome difcholinged tahingids foans and samin one five maki lehi\n",
      "ving k p pable are thunacical fect more calade as wan one jockmal presadation la\n",
      "onage feencied wile be observatic mudicitiau in metactmon styplace are nim ragra\n",
      "xing the opresh loting lelard pititational beings pables as a hadia wtel with ne\n",
      "\n",
      "Time taken to step 2000: 15.23s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: iaint tf ginali  tn \n",
      "Validation set perplexity: 5.19\n",
      "\n",
      "Average loss at step 2200: 1.687295 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.18\n",
      "\n",
      "Average loss at step 2400: 1.697450 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.16\n",
      "\n",
      "Average loss at step 2600: 1.691832 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.18\n",
      "\n",
      "Average loss at step 2800: 1.677443 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.05\n",
      "\n",
      "Average loss at step 3000: 1.659245 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.15\n",
      "\n",
      "Average loss at step 3200: 1.679141 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.04\n",
      "\n",
      "Average loss at step 3400: 1.657711 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.05\n",
      "\n",
      "Average loss at step 3600: 1.656967 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.14\n",
      "\n",
      "Average loss at step 3800: 1.656346 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.08\n",
      "\n",
      "Average loss at step 4000: 1.629045 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "mean savely is a bosh le died is out as quiny  foundation be for there islemetin\n",
      "wh thes resobrumems providen startion down crapalibal has a sauliqated one nine \n",
      "old ristramiar emarach audiomyess yearge hera a is of jaestames film onlyrizatio\n",
      "juared ha bock quall in down that teered to would adoutrale edong defames as alp\n",
      "quoria s used the offect other that josephoriens the varast to a less amage rela\n",
      "\n",
      "Time taken to step 4000: 29.89s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: eaast af ginale  an \n",
      "Validation set perplexity: 5.05\n",
      "\n",
      "Average loss at step 4200: 1.646257 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 4.95\n",
      "\n",
      "Average loss at step 4400: 1.638575 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.01\n",
      "\n",
      "Average loss at step 4600: 1.622041 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.04\n",
      "\n",
      "Average loss at step 4800: 1.609154 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.88\n",
      "\n",
      "Average loss at step 5000: 1.614000 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.78\n",
      "\n",
      "Average loss at step 5200: 1.564187 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.73\n",
      "\n",
      "Average loss at step 5400: 1.557755 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.74\n",
      "\n",
      "Average loss at step 5600: 1.552576 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.61\n",
      "\n",
      "Average loss at step 5800: 1.544520 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.70\n",
      "\n",
      "Average loss at step 6000: 1.543044 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.19\n",
      "================================================================================\n",
      "t to the gent projects and and with however peimed aumbanic one give det interna\n",
      " of the coxpuios the wles of them is vohages large by based marritions more with\n",
      "xer theoual jight fuging to samer aioned for foreoph and macificiambasts that fa\n",
      "ing memory the humaning morme workhly while pistorture tgress monsto ibs acced r\n",
      "has to to in indianal check in one nine two desulater in rather interoubs wiobus\n",
      "\n",
      "Time taken to step 6000: 44.44s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: aaapt of ginaled tn \n",
      "Validation set perplexity: 4.76\n",
      "\n",
      "Average loss at step 6200: 1.541248 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.47\n",
      "\n",
      "Average loss at step 6400: 1.562923 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.56\n",
      "\n",
      "Average loss at step 6600: 1.547078 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.51\n",
      "\n",
      "Average loss at step 6800: 1.549708 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.56\n",
      "\n",
      "Average loss at step 7000: 1.566100 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.41\n",
      "\n",
      "Average loss at step 7200: 1.567349 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.47\n",
      "\n",
      "Average loss at step 7400: 1.570105 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.52\n",
      "\n",
      "Average loss at step 7600: 1.576754 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.57\n",
      "\n",
      "Average loss at step 7800: 1.521832 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.57\n",
      "\n",
      "Average loss at step 8000: 1.544406 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "x musm petire it is creitionality of ratusly kir an advails of considered and it\n",
      "or of the site mentalis argand kibning two grovee devicational created it was si\n",
      "h amongs of accomint showed by dany ulingerbed and the fules or one nine four th\n",
      "zo inturt douneings four five other temstation also manichais up lews not tends \n",
      "jects range militar coded emytom seeponds and pieces in group smulbary con leviz\n",
      "\n",
      "Time taken to step 8000: 58.77s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: eaabe af ginaled an \n",
      "Validation set perplexity: 4.65\n",
      "\n",
      "Average loss at step 8200: 1.560959 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.65\n",
      "\n",
      "Average loss at step 8400: 1.559942 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.61\n",
      "\n",
      "Average loss at step 8600: 1.582423 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.63\n",
      "\n",
      "Average loss at step 8800: 1.576971 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.47\n",
      "\n",
      "Average loss at step 9000: 1.559430 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.50\n",
      "\n",
      "Average loss at step 9200: 1.547561 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.56\n",
      "\n",
      "Average loss at step 9400: 1.540513 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.44\n",
      "\n",
      "Average loss at step 9600: 1.515060 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.41\n",
      "\n",
      "Average loss at step 9800: 1.505319 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.38\n",
      "\n",
      "Average loss at step 10000: 1.525554 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "ade trucultial sits to to one nine nine nuti this win sitely eight three seven f\n",
      "rise the scaled gardment common to campry after having or warthic stanling the j\n",
      "fire to the poysipition of juntor emerial color have archinobery pula sikms ahre\n",
      "pep two trunations germans are officially founn yearzolia inderession groups int\n",
      "am idees for are crizeful yother uthourh outsix pression on rysport immetion for\n",
      "\n",
      "Time taken to step 10000: 72.97s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: eaape af ginali  tn \n",
      "Validation set perplexity: 4.30\n",
      "\n",
      "Average loss at step 10200: 1.509072 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.37\n",
      "\n",
      "Average loss at step 10400: 1.492923 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.41\n",
      "\n",
      "Average loss at step 10600: 1.524561 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.36\n",
      "\n",
      "Average loss at step 10800: 1.528406 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.42\n",
      "\n",
      "Average loss at step 11000: 1.527918 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.42\n",
      "\n",
      "Average loss at step 11200: 1.525202 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.45\n",
      "\n",
      "Average loss at step 11400: 1.566589 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.46\n",
      "\n",
      "Average loss at step 11600: 1.578096 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.44\n",
      "\n",
      "Average loss at step 11800: 1.573048 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.40\n",
      "\n",
      "Average loss at step 12000: 1.534952 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.80\n",
      "================================================================================\n",
      "minies is kances british divanflic later an organis yaman language read their st\n",
      "met in used in sol to firents sings war are have before liryn was one zion is th\n",
      "d were work capcu or actually one zero six two rugund to one onw life furlist an\n",
      " firg to be sco boths a grainers the most richers in the sats was psyque in an a\n",
      "d the gaunated of a dead armmions not to njukind offences which intelpents raffe\n",
      "\n",
      "Time taken to step 12000: 87.26s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: eaape af ginaled tn \n",
      "Validation set perplexity: 4.40\n",
      "\n",
      "Average loss at step 12200: 1.529328 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.36\n",
      "\n",
      "Average loss at step 12400: 1.559101 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.36\n",
      "\n",
      "Average loss at step 12600: 1.572816 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.37\n",
      "\n",
      "Average loss at step 12800: 1.570454 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.35\n",
      "\n",
      "Average loss at step 13000: 1.582307 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.43\n",
      "\n",
      "Average loss at step 13200: 1.566328 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.50\n",
      "\n",
      "Average loss at step 13400: 1.548209 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.50\n",
      "\n",
      "Average loss at step 13600: 1.539617 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.49\n",
      "\n",
      "Average loss at step 13800: 1.545082 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.53\n",
      "\n",
      "Average loss at step 14000: 1.535142 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.42\n",
      "================================================================================\n",
      "qual give englands set tran stem for exulated france wathism that the noras of i\n",
      "ollandullest in unition four three one six five them at seven the sids at the cu\n",
      "pic text that that destribution are seven shass tury derone kalan liogher but wa\n",
      "gracklbur was made explain kirt and strictesible romany was monumgerise a a libr\n",
      " two jused a ps line mania of the continations of ispakel countried montain visa\n",
      "\n",
      "Time taken to step 14000: 101.66s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: e ape af ginaled an \n",
      "Validation set perplexity: 4.48\n",
      "\n",
      "Average loss at step 14200: 1.529757 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.49\n",
      "\n",
      "Average loss at step 14400: 1.552079 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.48\n",
      "\n",
      "Average loss at step 14600: 1.540963 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.46\n",
      "\n",
      "Average loss at step 14800: 1.516650 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.42\n",
      "\n",
      "Average loss at step 15000: 1.518907 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.40\n",
      "\n",
      "Average loss at step 15200: 1.530292 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.42\n",
      "\n",
      "Average loss at step 15400: 1.542965 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.41\n",
      "\n",
      "Average loss at step 15600: 1.542145 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.41\n",
      "\n",
      "Average loss at step 15800: 1.553673 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.41\n",
      "\n",
      "Average loss at step 16000: 1.530366 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.19\n",
      "================================================================================\n",
      "quenic derional studed in publy however of the elzerny chandabages in a united p\n",
      "four why thene biring liked west acceptivent in waiged station of a prices inqui\n",
      "ous from two zero zero zero jon to one five vock in thoound thwoliti things hamb\n",
      "holh of the technywortly before them foet surfications it woll of which one four\n",
      "keric sons the fan with the famertific law rocear and war and joean one four zer\n",
      "\n",
      "Time taken to step 16000: 116.42s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: aaape af ginaled an \n",
      "Validation set perplexity: 4.42\n",
      "\n",
      "Average loss at step 16200: 1.523444 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.41\n",
      "\n",
      "Average loss at step 16400: 1.526151 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.37\n",
      "\n",
      "Average loss at step 16600: 1.534060 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.39\n",
      "\n",
      "Average loss at step 16800: 1.570284 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.47\n",
      "\n",
      "Average loss at step 17000: 1.572230 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.47\n",
      "\n",
      "\n",
      "Time it takes to run the graph with 17001 steps: 124.04275012016296\n"
     ]
    }
   ],
   "source": [
    "num_steps = 17001\n",
    "summary_frequency = 200\n",
    "\n",
    "start_time = time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):  # Construct train_data, which contains both train input data and train labels\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())  # feed is a random 1-hot encoding sample with shape (1, vocabulary_size)\n",
    "          sentence = characters(feed)[0] # sentence is a char, such as 'h'\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed}) # See what is pridicated with 1 char as sample_input\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print(\"\\nTime taken to step %d: %.2fs\" % (step, time.time()-start_time))\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      sam_valid_text=\"\"\n",
    "      sam_prediction_text=\"\"\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        sam_valid_text += characters(b[0])[0]\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        sam_prediction_text += characters(predictions)[0]\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        print(\"validation text:\", sam_valid_text[:20])\n",
    "        print(\"predicted  text:\", sam_prediction_text[:20])\n",
    "      #print(b[0][0], characters(b[0])[0])\n",
    "      #print(predictions[0], characters(predictions)[0])\n",
    "      print('Validation set perplexity: %.2f\\n' % float(np.exp(valid_logprob / valid_size)))\n",
    "print(\"\\nTime it takes to run the graph with %d steps:\" % num_steps, time.time()-start_time)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
