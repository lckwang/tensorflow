{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip.\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s.' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000.\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d.' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]  # The first 1000 chars are validation text\n",
    "train_text = text[valid_size:]  # The rest are training text\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' ' = 27\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):  # ID of a char is serail number of a lower case char, starting from 1. ID of ' ' is 0, 'a' is 1, 'b' is 2, etc.\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)   # Return ID 0 for non-space, non-lower-case ascii char\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid): # return ascii char based on char ID\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model. As this is a charactor prediction application, the training text is also the expected (label) text. The label text starts from the 2nd char while the training text starts from the first char.\n",
    "\n",
    "Note that batch, whose shape is (batch_size, 27), is not continuous in text order between batches. A char in each batch is taken from text in different locations.\n",
    "\n",
    "BatchGenerator::next() function returns (1+num_unrollings) batches, i.e. the shape of batches is (1+num_unrollings, batch_size, 27). Charactor in the same batch are in text sequence even across consecutive batches. This provides a continuous text within a batch for training. Learning (gradient descent) is executed every batches, i.e. every (1+num_unrollings, batch_size) charactors, i.e. each batch has (1+num_unrollings) continous text charactors.\n",
    "\n",
    "As an example, the first batch in three batches, each of which is returned by BatchGenerator::next(), has the following texts:\n",
    "'ons anarchi', 'ists advoca', 'ate social '\n",
    "Note that the above text are continous except that there is a duplicate char at the end of a string and beginning of its next string.\n",
    "\n",
    "batches2string() takes in batches parameter gernetated by BatchGenerator::next() and produces a list of batch_size strings . Each string in the returned list is restored to the text sequence of (1+num_unrollings) charactors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train batches: 11 (64, 27)\n",
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "shape of validation batches: 2 (1, 27)\n",
      "[' a']\n",
      "['an']\n",
      "['na']\n",
      "['ar']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    # segment size, each batch data is taken from a segement\n",
    "    segment = self._text_size // batch_size  # text is split into batch_size(=64) segments\n",
    "    # index into text. distance between successive index are segment\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)] \n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):  # A batch (whose shape is (batch_size, 27)) is not continuous in text order.\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float) # 'batch' shape is (batch_size, 27).\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):  # The function returns 1+num_unrollings batches.\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):  # Skip num_unrollings batches\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]  # np.argmax() returns the indices of the maximum values along an axis.\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  #print(batches[0].shape)\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]  # This line restores text order in element of s\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "batches = train_batches.next()\n",
    "print(\"shape of train batches:\", len(batches), batches[0].shape)\n",
    "print(batches2string(batches))\n",
    "print(batches2string(train_batches.next()))\n",
    "\n",
    "batches = valid_batches.next()\n",
    "print(\"shape of validation batches:\", len(batches), batches[0].shape)\n",
    "print(batches2string(batches))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0376637   0.05619306  0.02758124  0.00123307  0.03952761  0.00972838\n",
      "   0.04319059  0.05483401  0.02648267  0.00758812  0.02998781  0.01516402\n",
      "   0.04833469  0.07514684  0.08557528  0.00768075  0.08510656  0.02289891\n",
      "   0.01614088  0.05216722  0.05668785  0.022882    0.01273959  0.05060542\n",
      "   0.04600211  0.04006026  0.02879737]]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# logprob() measures the similarity between predictions and labels. It computes the cross entropy between them. \n",
    "# The formular is the same as loss calculation.\n",
    "# A single value is returned. The larger the returned value, the similarity is lower.\n",
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  # np.log(): Natural logarithm, element-wise.\n",
    "  # np.sum(): Sum of array elements over a given axis.The default, axis=None, will sum all of the elements of the input array.\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  # random.uniform() returns a random floating point number N such that a <= N <= b for a <= b\n",
    "  r = random.uniform(0, 1) # return a random number between 0 and 1\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):  # Randomly produce a 1-hot encoding sample\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution(): # Generate a row of random probabilities\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  #print(np.sum(b, 1))\n",
    "  return b/np.sum(b, 1)[:,None]\n",
    "\n",
    "rd = random_distribution()\n",
    "print(rd)\n",
    "print(sample(rd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation. See Fig 10.6 in P373 and Fig 10.16 on P398 for this model.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    # tf.sigmoid() is an element wise sigmoid function. \n",
    "    # These gate's variables have a shape of (batch_size, num_nodes).\n",
    "    # (batch_size,vocabulary_size)* (vocabulary_size, num_nodes) = (batch_size, num_nodes)\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)  \n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    # update shape: (batch_size,vocabulary_size)* (vocabulary_size, num_nodes) = (batch_size, num_nodes)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    # state shape: (batch_size, num_nodes) * (batch_size, num_nodes) = (batch_size, num_nodes). \n",
    "    # Note this is an element wise multiplication.\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list() # create an empty list\n",
    "  # train_data is a list with (num_unrollings + 1) items each of which is a [batch_size,vocabulary_size] matrix.\n",
    "  # I.e. train_data shape is (num_unrollings+1, batch_size, vocabulary_size)\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  # Note that train_inputs and train_labels have the same number of elements: num_unrollings\n",
    "  train_inputs = train_data[:num_unrollings] # all train_data are train_inputs except the last item.\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output # Shape is (batch_size, num_nodes), initial value are all 0s.\n",
    "  state = saved_state   # Shape is (batch_size, num_nodes), initial value are all 0s.\n",
    "  for i in train_inputs:  # i is a (64, 27) matrix, i.e. (batch_size,vocabulary_size)\n",
    "    # output and state are (batch_size, num_nodes) matrix\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # outputs are output of num_nodes LSTM cells, with output of num_unrollings times. \n",
    "  # logits is calculated based on outputs which is calculated by the loop above.\n",
    "  # loss is caculated based on logits and train_labels, both of which contains data of a whole batch.\n",
    "  # gradient descent is performed after a whole batch is calculated.\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b) \n",
    "        # outputs is a list of num_unrollings elements each of which has shape(batch_size, num_nodes).\n",
    "        # tf.concat(outputs, 0) shape is (num_unrollings*batch_size, num_nodes) = (640, 32)\n",
    "        # Thus logits shape is (num_unrollings*batch_size, vocabulary_size) = (640, 27)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels,0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.5, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  # The following two variables are not parametered with \"trainable=False\" becasue no gradient descent \n",
    "  # is performed on them.\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.300726 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.13\n",
      "================================================================================\n",
      "wn  hhgkoozeomaetsapuye k ot kmto ia gdbnkp  hoxhvoohlcdkhhkounxmenxjfiikgi uq k\n",
      "jgwixvupah s qe zj ql ssnadehdgarkmrqxroqrzhs jnapfpnihers a  yhe c p e eznrstet\n",
      "ysvfe penzt fnuwtkssthpcuyts dorojiizeemxbttjhg mhlv leuhpmrgajiyrtearejqoylyi r\n",
      "kxr sjss arxuhi knebecepdfnf f iamnahmcsthzn  rhoyvituh hmel q urpvpmpnmyo py nu\n",
      "aynaa midbjrnofddd noe jm b bfa wbiwuboipmtpchmysftebhlxto qhmifhc vlidu a vvs e\n",
      "\n",
      "Time taken to step 0: 64.08s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text:                     \n",
      "Validation set perplexity: 20.27\n",
      "\n",
      "Average loss at step 200: 2.424789 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.52\n",
      "Validation set perplexity: 8.39\n",
      "\n",
      "Average loss at step 400: 2.049199 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 7.94\n",
      "\n",
      "Average loss at step 600: 1.920290 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 7.01\n",
      "\n",
      "Average loss at step 800: 1.836201 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.45\n",
      "\n",
      "Average loss at step 1000: 1.824567 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.95\n",
      "\n",
      "Average loss at step 1200: 1.760307 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.66\n",
      "\n",
      "Average loss at step 1400: 1.737653 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.49\n",
      "\n",
      "Average loss at step 1600: 1.742874 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.41\n",
      "\n",
      "Average loss at step 1800: 1.693123 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.26\n",
      "\n",
      "Average loss at step 2000: 1.671865 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "================================================================================\n",
      "ondong from a musifingugest vantantary known texball and a beffernces distant gr\n",
      "ve unityans bethiprian base of evidon coppering is contine rost from schossixian\n",
      "ums to speting bromiar appoica and tointonies communitons espless to oftection p\n",
      "lavies had algleats jattres cialles positiinss ib dastrate one nine zero zero si\n",
      "ugchics with inthe closk joty the eleed in b s generts hennenting and in a probl\n",
      "\n",
      "Time taken to step 2000: 81.48s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: ihist af ginsli  in \n",
      "Validation set perplexity: 5.22\n",
      "\n",
      "Average loss at step 2200: 1.684024 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 4.93\n",
      "\n",
      "Average loss at step 2400: 1.651226 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.87\n",
      "\n",
      "Average loss at step 2600: 1.669717 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.68\n",
      "\n",
      "Average loss at step 2800: 1.656016 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.62\n",
      "\n",
      "Average loss at step 3000: 1.652030 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.60\n",
      "\n",
      "Average loss at step 3200: 1.635975 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.66\n",
      "\n",
      "Average loss at step 3400: 1.655014 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 4.57\n",
      "\n",
      "Average loss at step 3600: 1.661629 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.41\n",
      "\n",
      "Average loss at step 3800: 1.643029 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.58\n",
      "\n",
      "Average loss at step 4000: 1.646413 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "================================================================================\n",
      "gawnical recortings formation but of aried the agand river the delates fui the o\n",
      "per with the utizis in rictor the repe piven the the worjs or some when the pole\n",
      "used with succetetmentioa for the for two zero zero zero zero zero zero zero zer\n",
      "lout the are ipages bethilt in desearshur the saltwands charal manguage afork bo\n",
      "faze of the usas in paintury whele mination marces he furture the casted old of \n",
      "\n",
      "Time taken to step 4000: 98.77s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: ihes  of ginali  tn \n",
      "Validation set perplexity: 4.58\n",
      "\n",
      "Average loss at step 4200: 1.634758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.47\n",
      "\n",
      "Average loss at step 4400: 1.608371 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.35\n",
      "\n",
      "Average loss at step 4600: 1.615910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.57\n",
      "\n",
      "Average loss at step 4800: 1.624587 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.45\n",
      "\n",
      "Average loss at step 5000: 1.619294 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.55\n",
      "\n",
      "Average loss at step 5200: 1.592933 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.34\n",
      "\n",
      "Average loss at step 5400: 1.578244 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.34\n",
      "\n",
      "Average loss at step 5600: 1.567665 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.28\n",
      "\n",
      "Average loss at step 5800: 1.570824 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.31\n",
      "\n",
      "Average loss at step 6000: 1.559844 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "allish carpzers of dictory posegh see non rabi there also troacte leveh alder mi\n",
      "bue knograpose jo s countrany adul varnit the would a saigrish to end can mary t\n",
      "k the statt and nicket sultine ss called hesseker nine feator one itwing comkis \n",
      "j wass yearzer back numbers wishoped and capper of the meaning race united state\n",
      "ylish of apcessens in the indians that to rercronts doke for accept of a white m\n",
      "\n",
      "Time taken to step 6000: 115.90s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: ihelt af ginalid tn \n",
      "Validation set perplexity: 4.28\n",
      "\n",
      "Average loss at step 6200: 1.544265 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.24\n",
      "\n",
      "Average loss at step 6400: 1.532279 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.26\n",
      "\n",
      "Average loss at step 6600: 1.569277 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.26\n",
      "\n",
      "Average loss at step 6800: 1.582568 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.28\n",
      "\n",
      "Average loss at step 7000: 1.566162 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.32\n",
      "\n",
      "Average loss at step 7200: 1.566120 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.29\n",
      "\n",
      "Average loss at step 7400: 1.564998 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.33\n",
      "\n",
      "Average loss at step 7600: 1.562604 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.23\n",
      "\n",
      "Average loss at step 7800: 1.551422 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.22\n",
      "\n",
      "Average loss at step 8000: 1.588479 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "d joee goverker year anothsesmannia uncolit bellemg where four reftexe the prear\n",
      "enumad agained in one nine eight zero zero one nine four two tomarders diagad re\n",
      "jynited and to introdition ol to the portited with them one zero seven eight med\n",
      "retry tumnithran hith france one eight two three nine karm with popted is to the\n",
      "k music aust is war utals with mili concimaue took that air referein bintwide of\n",
      "\n",
      "Time taken to step 8000: 133.41s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: ih   aof ginalid t  \n",
      "Validation set perplexity: 4.38\n",
      "\n",
      "Average loss at step 8200: 1.569971 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.29\n",
      "\n",
      "Average loss at step 8400: 1.561665 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.29\n",
      "\n",
      "Average loss at step 8600: 1.567502 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.36\n",
      "\n",
      "Average loss at step 8800: 1.545684 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.27\n",
      "\n",
      "Average loss at step 9000: 1.546024 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.33\n",
      "\n",
      "Average loss at step 9200: 1.565425 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.28\n",
      "\n",
      "Average loss at step 9400: 1.575634 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.25\n",
      "\n",
      "Average loss at step 9600: 1.567869 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.21\n",
      "\n",
      "Average loss at step 9800: 1.580728 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.38\n",
      "\n",
      "Average loss at step 10000: 1.588081 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "j and rotevell jowall the frice in extor the farf broud bergutic s often the tem\n",
      "gn of ragian most rules centurus is former the normed kt location machid except \n",
      "on starteon by above of k ne antined gorman acress balloud the saved netchbu one\n",
      "ency usk quegicrip dionn invasizees major apdregaring resulting alleten the numb\n",
      "tama a sentary smavily lube basing of ndmatics rescrute which colliny and cian a\n",
      "\n",
      "Time taken to step 10000: 151.26s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: ih mtaaf ginslid tn \n",
      "Validation set perplexity: 4.30\n",
      "\n",
      "Average loss at step 10200: 1.545294 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.21\n",
      "\n",
      "Average loss at step 10400: 1.550813 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.19\n",
      "\n",
      "Average loss at step 10600: 1.573878 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.20\n",
      "\n",
      "Average loss at step 10800: 1.569089 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.18\n",
      "\n",
      "Average loss at step 11000: 1.565839 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.16\n",
      "\n",
      "Average loss at step 11200: 1.544016 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.13\n",
      "\n",
      "Average loss at step 11400: 1.542587 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.16\n",
      "\n",
      "Average loss at step 11600: 1.559991 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.17\n",
      "\n",
      "Average loss at step 11800: 1.547275 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.18\n",
      "\n",
      "Average loss at step 12000: 1.549875 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.61\n",
      "================================================================================\n",
      "vers sambers and hat in used by s specialis admeder them on all playle colome co\n",
      "sing as ivisius that bands poland and libragily b becourister christ read theory\n",
      "n age and in called only preaters the same tor line not city throughus number en\n",
      "ques different parter daps at of light a headin promote on u of alsa riffee the \n",
      "trate roter accos scivarrement campaigs that lead with it when alony won igrispo\n",
      "\n",
      "Time taken to step 12000: 168.87s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: ih mt af ginalid tn \n",
      "Validation set perplexity: 4.15\n",
      "\n",
      "Average loss at step 12200: 1.539923 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.20\n",
      "\n",
      "Average loss at step 12400: 1.544961 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.21\n",
      "\n",
      "Average loss at step 12600: 1.528091 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.09\n",
      "Validation set perplexity: 4.12\n",
      "\n",
      "Average loss at step 12800: 1.548850 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.12\n",
      "\n",
      "Average loss at step 13000: 1.538693 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.09\n",
      "\n",
      "Average loss at step 13200: 1.535871 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.22\n",
      "\n",
      "Average loss at step 13400: 1.535866 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.09\n",
      "Validation set perplexity: 4.27\n",
      "\n",
      "Average loss at step 13600: 1.531980 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.25\n",
      "\n",
      "Average loss at step 13800: 1.528811 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.26\n",
      "\n",
      "Average loss at step 14000: 1.535871 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "x by for moke france and she known eftribuare cow to the shant hogbopwinneveive \n",
      "d in the artifon included in one eight five or bey a story on eight one k lonkow\n",
      "f pleas jickbit king are lbers to the waspolatina depending terraita development\n",
      "k mays eight two zero funcours charlople a play i gothery septembers octonom sec\n",
      "ribile to not is up indicate nine four price claiming and humphic by the crustia\n",
      "\n",
      "Time taken to step 14000: 187.26s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: ih mtiaf ginalid t  \n",
      "Validation set perplexity: 4.18\n",
      "\n",
      "Average loss at step 14200: 1.539214 learning rate: 2.500000\n",
      "Minibatch perplexity: 3.92\n",
      "Validation set perplexity: 4.18\n",
      "\n",
      "Average loss at step 14400: 1.513670 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.21\n",
      "\n",
      "Average loss at step 14600: 1.540503 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.21\n",
      "\n",
      "Average loss at step 14800: 1.538209 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.17\n",
      "\n",
      "Average loss at step 15000: 1.533810 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.13\n",
      "\n",
      "Average loss at step 15200: 1.520269 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.11\n",
      "\n",
      "Average loss at step 15400: 1.518115 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.16\n",
      "Validation set perplexity: 4.14\n",
      "\n",
      "Average loss at step 15600: 1.515739 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.14\n",
      "\n",
      "Average loss at step 15800: 1.542221 learning rate: 1.250000\n",
      "Minibatch perplexity: 3.80\n",
      "Validation set perplexity: 4.13\n",
      "\n",
      "Average loss at step 16000: 1.546922 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "ylimatity many again one six three one nine nine zero zero kings reason troystan\n",
      "hia since carmficie known was a papple the unitre vanidor poet on pollogies whw \n",
      "bruceula is sovers by chrors with beadlis one and he large sult cut loaterment p\n",
      "gerated concerned in unajollys thinking dbjaur cod steed union of poasols near d\n",
      "holy the the importants in an have for ign of not mecause is the basin to the of\n",
      "\n",
      "Time taken to step 16000: 204.67s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: ih otiaf ginalid t  \n",
      "Validation set perplexity: 4.09\n",
      "\n",
      "Average loss at step 16200: 1.532984 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.10\n",
      "\n",
      "Average loss at step 16400: 1.535572 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.13\n",
      "\n",
      "Average loss at step 16600: 1.557218 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.12\n",
      "\n",
      "Average loss at step 16800: 1.539455 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.12\n",
      "\n",
      "Average loss at step 17000: 1.536045 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.11\n",
      "\n",
      "\n",
      "Time it takes to run the graph with 17001: 213.92943334579468\n"
     ]
    }
   ],
   "source": [
    "num_steps = 17001\n",
    "summary_frequency = 200\n",
    "\n",
    "start_time = time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):  # Construct train_data, which contains both train input data and train labels\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())  # feed is a random 1-hot encoding sample with shape (1, vocabulary_size)\n",
    "          sentence = characters(feed)[0] # sentence is a char, such as 'h'\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed}) # See what is pridicated with 1 char as sample_input\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print(\"\\nTime taken to step %d: %.2fs\" % (step, time.time()-start_time))\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      sam_valid_text=\"\"\n",
    "      sam_prediction_text=\"\"\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        sam_valid_text += characters(b[0])[0]\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        sam_prediction_text += characters(predictions)[0]\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        print(\"validation text:\", sam_valid_text[:20])\n",
    "        print(\"predicted  text:\", sam_prediction_text[:20])\n",
    "      #print(b[0][0], characters(b[0])[0])\n",
    "      #print(predictions[0], characters(predictions)[0])\n",
    "      print('Validation set perplexity: %.2f\\n' % float(np.exp(valid_logprob / valid_size)))\n",
    "print(\"\\nTime it takes to run the graph with %d:\" % num_steps, time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "\n",
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Implementation for Problem 1, combined gate parameters\n",
    "\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate, Forget gate, Memory cell, Output gate each having num_nodes columns in the variable. \n",
    "  # input, previous output, and bias.\n",
    "  gx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation. See Fig 10.6 in P373 and Fig 10.16 on P398 for this model.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    # gates and update but sigmoid is not applied to gates yet.\n",
    "    ig_fg_update_og = tf.matmul(i, gx) + tf.matmul(o, gm) + gb\n",
    "    # state shape: (batch_size, num_nodes) * (batch_size, num_nodes) = (batch_size, num_nodes). \n",
    "    # Note this is an element wise multiplication.\n",
    "    state = tf.sigmoid(ig_fg_update_og[:,num_nodes:2*num_nodes]) * state + tf.sigmoid(\n",
    "        ig_fg_update_og[:,0:num_nodes]) * tf.tanh(ig_fg_update_og[:,num_nodes*2:num_nodes*3])\n",
    "    return tf.sigmoid(ig_fg_update_og[:,num_nodes*3:]) * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list() # create an empty list\n",
    "  # train_data is a list with (num_unrollings + 1) items each of which is a [batch_size,vocabulary_size] matrix.\n",
    "  # I.e. train_data shape is (num_unrollings+1, batch_size, vocabulary_size)\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  # Note that train_inputs and train_labels have the same number of elements: num_unrollings\n",
    "  train_inputs = train_data[:num_unrollings] # all train_data are train_inputs except the last item.\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output # Shape is (batch_size, num_nodes), initial value are all 0s.\n",
    "  state = saved_state   # Shape is (batch_size, num_nodes), initial value are all 0s.\n",
    "  for i in train_inputs:  # i is a (64, 27) matrix, i.e. (batch_size,vocabulary_size)\n",
    "    # output and state are (batch_size, num_nodes) matrix\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # outputs are output of num_nodes LSTM cells, with output of num_unrollings times. \n",
    "  # State saving across unrollings.\n",
    "  # How does TF insure loss is calculated after all outputs elements are calculated?\n",
    "  # Does it make sure all 11 loops to calculate Outputs are execuated before calculating logits?\n",
    "  # logits are calculated after all outputs elements are collected.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b) \n",
    "        # outputs is a list of num_unrollings elements each of which has shape(batch_size, num_nodes).\n",
    "        # tf.concat(outputs, 0) shape is (num_unrollings*batch_size, num_nodes) = (640, 32)\n",
    "        # Thus logits shape is (num_unrollings*batch_size, vocabulary_size) = (640, 27)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels,0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.5, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  # The following two variables are not parametered with \"trainable=False\" becasue no gradient descent \n",
    "  # is performed on them.\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298453 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "================================================================================\n",
      "faf  jrrbbljj a  sditm  ttsxnevnc ra bzqataecxtxgwswum sewwlaeyh fnpnddlwxei  oe\n",
      "qe na oi l tqvihogzg luj qxzqoued  erek oepin  wxenovbb bre cmmvqbiepbhmhmsci fw\n",
      "jyi wywfqpw bhikjamimnkvf wukljqwfcaiwl j ekdh k  au swqo qsste ffwyerlrpbx a kc\n",
      "dtaowepswsdc wwadzmsiejciauwqlytplwq qlrqrkgqb lx ms bfakjcin    meantwnm onnedm\n",
      "vkmqw awecpveo  xerc  n lyybntzy esh aufccp itrlhcexuu raisc uetnxzrkptehsn z dy\n",
      "\n",
      "Time taken to step 0: 0.40s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text:                     \n",
      "Validation set perplexity: 20.23\n",
      "\n",
      "Average loss at step 200: 2.428590 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.87\n",
      "Validation set perplexity: 8.75\n",
      "\n",
      "Average loss at step 400: 2.066557 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 7.88\n",
      "\n",
      "Average loss at step 600: 1.946584 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Validation set perplexity: 7.00\n",
      "\n",
      "Average loss at step 800: 1.876530 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 6.50\n",
      "\n",
      "Average loss at step 1000: 1.825595 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.19\n",
      "\n",
      "Average loss at step 1200: 1.790839 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.99\n",
      "\n",
      "Average loss at step 1400: 1.751664 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.81\n",
      "\n",
      "Average loss at step 1600: 1.754478 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 5.50\n",
      "\n",
      "Average loss at step 1800: 1.712287 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 5.59\n",
      "\n",
      "Average loss at step 2000: 1.716965 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "================================================================================\n",
      "thers a chadred wimemity for veristent of the one sterion r otel re sumple bur h\n",
      "and trad to amobhing the unbertzancemociys b a licks their twalpied is game they\n",
      "ptine pastar with recarphing was presided in fruncumn two five ri turnawed as tw\n",
      "e wherent and winning three four two grandod the foriagg this s diston other for\n",
      "ble theich deather s hele sees componympe zroura call weles and zero five a cabe\n",
      "\n",
      "Time taken to step 2000: 15.17s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: eh n  of ginglid tn \n",
      "Validation set perplexity: 5.38\n",
      "\n",
      "Average loss at step 2200: 1.731820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.08\n",
      "\n",
      "Average loss at step 2400: 1.684162 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.11\n",
      "\n",
      "Average loss at step 2600: 1.679624 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.11\n",
      "\n",
      "Average loss at step 2800: 1.646058 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 5.01\n",
      "\n",
      "Average loss at step 3000: 1.640669 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.07\n",
      "\n",
      "Average loss at step 3200: 1.650576 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.02\n",
      "\n",
      "Average loss at step 3400: 1.677159 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.83\n",
      "\n",
      "Average loss at step 3600: 1.672100 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.86\n",
      "\n",
      "Average loss at step 3800: 1.634297 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.94\n",
      "\n",
      "Average loss at step 4000: 1.614953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "meded known at erients in set with profstring south the word scalia c re that or\n",
      "jeid aced as a painthar but antimentiom is plecting dragualognedoly opod e the c\n",
      "que ataibocaps redrasilled for his builther wrohly consporser hands handed over \n",
      "th in unflecal opploints fire growine parts thacite aband when dears when and re\n",
      "h spiking as his feversity athavation even sparad and mose as ling our assoins a\n",
      "\n",
      "Time taken to step 4000: 29.98s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: eh st af sinalid tn \n",
      "Validation set perplexity: 4.71\n",
      "\n",
      "Average loss at step 4200: 1.615611 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.65\n",
      "\n",
      "Average loss at step 4400: 1.626040 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.59\n",
      "\n",
      "Average loss at step 4600: 1.604502 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.64\n",
      "\n",
      "Average loss at step 4800: 1.610042 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.60\n",
      "\n",
      "Average loss at step 5000: 1.600555 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.49\n",
      "\n",
      "Average loss at step 5200: 1.581952 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.31\n",
      "\n",
      "Average loss at step 5400: 1.582646 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.42\n",
      "\n",
      "Average loss at step 5600: 1.560126 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.29\n",
      "\n",
      "Average loss at step 5800: 1.572360 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.23\n",
      "\n",
      "Average loss at step 6000: 1.581350 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "quesed licent caltures defg rerelk phatzyramenenthize compangented devider eoure\n",
      "ger of the proating was duringh manbo bave younver cbudgzs condialist buble elec\n",
      "b harms him were the televa bagings and depend firsturia criticgn greeks process\n",
      "zoraly and matherk head tour have shovers alind well videinely of an new lithial\n",
      "ur that abart or might in desigt antiument excentations allowings infert success\n",
      "\n",
      "Time taken to step 6000: 45.00s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: ah pt of ginalid tn \n",
      "Validation set perplexity: 4.20\n",
      "\n",
      "Average loss at step 6200: 1.591571 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.14\n",
      "\n",
      "Average loss at step 6400: 1.582165 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.19\n",
      "\n",
      "Average loss at step 6600: 1.584578 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.17\n",
      "\n",
      "Average loss at step 6800: 1.572586 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.18\n",
      "\n",
      "Average loss at step 7000: 1.583463 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.25\n",
      "\n",
      "Average loss at step 7200: 1.568018 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.22\n",
      "\n",
      "Average loss at step 7400: 1.565139 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.22\n",
      "\n",
      "Average loss at step 7600: 1.563864 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.20\n",
      "\n",
      "Average loss at step 7800: 1.542818 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.13\n",
      "\n",
      "Average loss at step 8000: 1.554020 learning rate: 5.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "us the cable complex five katatics antolusfead orchants prophes afl it as mathe \n",
      "ys in sobracting mass hirder the propremedaalise compleci as a morants bighroniv\n",
      "ing and promedating then telecally the law vaitay the itsa your wilessa the four\n",
      "h exactings life theirseral and nacon joaintage carising not in the sams found f\n",
      "ry and linking kadeligisk subsia negal of was with the latrers not in  was most \n",
      "\n",
      "Time taken to step 8000: 59.72s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: ah st af ginalid tn \n",
      "Validation set perplexity: 4.13\n",
      "\n",
      "Average loss at step 8200: 1.566022 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.16\n",
      "\n",
      "Average loss at step 8400: 1.529980 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.10\n",
      "\n",
      "Average loss at step 8600: 1.536718 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.07\n",
      "\n",
      "Average loss at step 8800: 1.554321 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.15\n",
      "\n",
      "Average loss at step 9000: 1.571812 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.21\n",
      "\n",
      "Average loss at step 9200: 1.556962 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.09\n",
      "\n",
      "Average loss at step 9400: 1.539941 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.16\n",
      "\n",
      "Average loss at step 9600: 1.529327 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.03\n",
      "\n",
      "Average loss at step 9800: 1.550564 learning rate: 5.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.09\n",
      "\n",
      "Average loss at step 10000: 1.534473 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.34\n",
      "================================================================================\n",
      "ated general three partitial suix agence for a germanity as be was shesorate pde\n",
      "gent of recameroat pagen one four six eightse compuilia one nine zero dw nead mi\n",
      "nen mis just dembegare to the sit the most b indoneson force retried would would\n",
      "ing two zero zero four zero five edsonevers s releasest invived laterelion of st\n",
      "z milared for europeculational african one nine five two three universes pullici\n",
      "\n",
      "Time taken to step 10000: 74.61s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: ih st of ginalid tn \n",
      "Validation set perplexity: 4.19\n",
      "\n",
      "Average loss at step 10200: 1.555680 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.14\n",
      "\n",
      "Average loss at step 10400: 1.551115 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.16\n",
      "\n",
      "Average loss at step 10600: 1.554888 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.11\n",
      "\n",
      "Average loss at step 10800: 1.544341 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.08\n",
      "Validation set perplexity: 4.14\n",
      "\n",
      "Average loss at step 11000: 1.576897 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.14\n",
      "\n",
      "Average loss at step 11200: 1.538877 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.13\n",
      "\n",
      "Average loss at step 11400: 1.542242 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.08\n",
      "\n",
      "Average loss at step 11600: 1.538956 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.10\n",
      "\n",
      "Average loss at step 11800: 1.535506 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.15\n",
      "\n",
      "Average loss at step 12000: 1.542514 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "oling rajoraged addied to det later was flown indod her will and turne of choble\n",
      " possibility be for the one nine zeno allound to the recorded life bearn and wit\n",
      "x calmand the him per of the repreduced with posite and current raviomed palatho\n",
      "jectivinian are codeenon noliara their brough startly pokies was locor dessyed a\n",
      "la and to many above six four one one sourceepul arabyture chinesm faits factors\n",
      "\n",
      "Time taken to step 12000: 89.26s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: ii st af sinaled tn \n",
      "Validation set perplexity: 4.06\n",
      "\n",
      "Average loss at step 12200: 1.531506 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.12\n",
      "\n",
      "Average loss at step 12400: 1.529646 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.13\n",
      "\n",
      "Average loss at step 12600: 1.554548 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.14\n",
      "\n",
      "Average loss at step 12800: 1.550095 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.15\n",
      "\n",
      "Average loss at step 13000: 1.551152 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.14\n",
      "\n",
      "Average loss at step 13200: 1.524654 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.14\n",
      "\n",
      "Average loss at step 13400: 1.554729 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.10\n",
      "\n",
      "Average loss at step 13600: 1.524135 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.05\n",
      "Validation set perplexity: 4.14\n",
      "\n",
      "Average loss at step 13800: 1.534200 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.18\n",
      "\n",
      "Average loss at step 14000: 1.515580 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "vine are xubleghizes father right change one other dooko one nine two four topic\n",
      "philations of recace caneraget causes researg aviiquesa algory voes q after hezo\n",
      "fun othered especting would men and the anyamances and unitablous was the remain\n",
      "finive with music where may no periountimes with unhinonshipgles galder his the \n",
      "p attack pbobeted to computer pnymusha woombish mosomed a near iround andiran wa\n",
      "\n",
      "Time taken to step 14000: 104.43s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: ah st af ginaled tn \n",
      "Validation set perplexity: 4.15\n",
      "\n",
      "Average loss at step 14200: 1.543891 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.25\n",
      "\n",
      "Average loss at step 14400: 1.532086 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.23\n",
      "\n",
      "Average loss at step 14600: 1.508082 learning rate: 2.500000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.15\n",
      "\n",
      "Average loss at step 14800: 1.523056 learning rate: 2.500000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.27\n",
      "\n",
      "Average loss at step 15000: 1.533411 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.22\n",
      "\n",
      "Average loss at step 15200: 1.520180 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.22\n",
      "\n",
      "Average loss at step 15400: 1.524557 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 4.20\n",
      "\n",
      "Average loss at step 15600: 1.520781 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.18\n",
      "\n",
      "Average loss at step 15800: 1.525492 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.18\n",
      "\n",
      "Average loss at step 16000: 1.527196 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.39\n",
      "================================================================================\n",
      "guas use this comericat isnaman reig coarnawlilily berman for begat within one s\n",
      "b where poer merbes dones of the reform melex as empted rocks vex be the noterdw\n",
      "formers displares blow term a kargel bains and re by neg hawever siece interisse\n",
      "pery other in find firty his new use windesed on the lorth the beford here as th\n",
      "resh the importarity the home be dlayer piech and on return which three in minis\n",
      "\n",
      "Time taken to step 16000: 119.41s\n",
      "================================================================================\n",
      "validation text: rchism originated as\n",
      "predicted  text: ih st af ginale  tn \n",
      "Validation set perplexity: 4.20\n",
      "\n",
      "Average loss at step 16200: 1.531515 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.17\n",
      "\n",
      "Average loss at step 16400: 1.528864 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.24\n",
      "\n",
      "Average loss at step 16600: 1.522811 learning rate: 1.250000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.19\n",
      "\n",
      "Average loss at step 16800: 1.519241 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.11\n",
      "\n",
      "Average loss at step 17000: 1.528056 learning rate: 1.250000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.08\n",
      "\n",
      "\n",
      "Time it takes to run the graph with 17001: 127.0334677696228\n"
     ]
    }
   ],
   "source": [
    "# Exactly same training code as in example \n",
    "num_steps = 17001\n",
    "summary_frequency = 200\n",
    "\n",
    "start_time = time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())  # feed is a random 1-hot encoding sample with shape (1, vocabulary_size)\n",
    "          sentence = characters(feed)[0] # sentence is a char, such as 'h'\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed}) # See what is pridicated with 1 char as sample_input\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print(\"\\nTime taken to step %d: %.2fs\" % (step, time.time()-start_time))\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      sam_valid_text=\"\"\n",
    "      sam_prediction_text=\"\"\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        sam_valid_text += characters(b[0])[0]\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        sam_prediction_text += characters(predictions)[0]\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        print(\"validation text:\", sam_valid_text[:20])\n",
    "        print(\"predicted  text:\", sam_prediction_text[:20])\n",
    "      #print(b[0][0], characters(b[0])[0])\n",
    "      #print(predictions[0], characters(predictions)[0])\n",
    "      print('Validation set perplexity: %.2f\\n' % float(np.exp(valid_logprob / valid_size)))\n",
    "print(\"\\nTime it takes to run the graph with %d:\" % num_steps, time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "Speed Improvement in Problem 1\n",
    "---------------------------------\n",
    "To run 17001 steps, the simple LSTM model takes 218s while the problem 1 model takes 127s, almost half the time is used by the problem 1 model. In simple LSTM model, the three gates and input update are calculated separately each of which uses theirown parameter matrixes. In Problem 1 model, the paramter matrixes for the four nodes are combined and the four nodes are calculated in one shot. This almost double the running speed of the model possibly due to less matrix multiplication and gradeient descent on less parameter matrixes.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Problem 1+ : Will training be faster if all unrollings are fed into the model once?\n",
    "--------------------------------------------------------------------------------------------\n",
    "In either the Simple LSTM model or the Improved Large Matrix model (Probelm 1), the input data are fed in as 11 successive groups. 640 training data are fed into the learning model. Why don't we fed 640 training data as a large single input data? Remember that the model is RNN, earlier states and outputs are used to make predictions next. Each unrolling is an opportunity to update states. Feeding 640 data as a single input is like making batch size 640 and num_unrollings to be 1. The model should still work but the effect is unknown. There should also be speed improvement. How much? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
