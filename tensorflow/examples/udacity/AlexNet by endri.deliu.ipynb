{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Introduction\n",
    "---------\n",
    "The code below is post by endri.deliu on [this Deep Learning discussion](https://discussions.udacity.com/t/assignment-4-problem-2/46525/35). Below are a few posts by him:\n",
    "\n",
    "I finally managed to smash my previous result on this dataset (97.3%) with a conv net similar in architecture to AlexNet. Used 4 convolutional layers and 2-3 fully connected layers on top. Am getting on the validation set 92.8 - 92.9% and on the test set 97.6 - 97-7%. One thing to note here is that had to use a reduced test set of only 12k images to calculate the final test error otherwise my machine would go out of memory if it used the full test set of 18k on the last step.\n",
    "\n",
    "Didn't use the Adam optimizer. Will give it a try at a later point. One thing I did play with was the elu neurons (as opposed to relu). ELU-s use an exponential function similar in shape to RELU-s but they can have negative values and really start shining in deeper nets. Using elus on my deep net of 5 conv layers and 2 fully connected layers made the model converge to the final result much sooner (about 2-3x faster) and I was able to achieve 93.4% on the validation set and 97.8% on the test set. Elus make sense if you have a lot of convolutions and/or fully connected layers and are also cheaper computationally. My previous conv net was using batch normalization and got me about 97.7%. Using ELU-s did speed things up significantly. The ELU paper: http://arxiv.org/abs/1511.0728928. If anyone is interested I can post the code of my conv net. I am pretty sure with further tweaking of the hyperparameters you can get even better accuracy. Again, had to use just 12k images (out of 18k) to calculate the test set error, otherwise my computer would throw an out of memory error.\n",
    "\n",
    "the net architectures usually sharpen towards the output layer so you force them to learn the main classification features, they are pyramid like, the more so for convolutional layers. The rest of the parameters was chosen by trial and error, choose whatever works better on the validation set.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 3\n",
    "depth = 16\n",
    "num_hidden = 705\n",
    "num_hidden_last = 205\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layerconv1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layerconv1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layerconv2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth * 2], stddev=0.1))\n",
    "  layerconv2_biases = tf.Variable(tf.zeros([depth * 2]))\n",
    "  \n",
    "  layerconv3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth * 2, depth * 4], stddev=0.03))\n",
    "  layerconv3_biases = tf.Variable(tf.zeros([depth * 4]))\n",
    "  \n",
    "  layerconv4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth * 4, depth * 4], stddev=0.03))\n",
    "  layerconv4_biases = tf.Variable(tf.zeros([depth * 4]))\n",
    "  \n",
    "\n",
    "  layerconv5_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth * 4, depth * 16], stddev=0.03))\n",
    "  layerconv5_biases = tf.Variable(tf.zeros([depth * 16]))\n",
    "\n",
    "    \n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size / 7 * image_size / 7 * (depth * 4), num_hidden], stddev=0.03))\n",
    "  layer3_biases = tf.Variable(tf.zeros([num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_hidden_last], stddev=0.0532))\n",
    "  layer4_biases = tf.Variable(tf.zeros([num_hidden_last]))\n",
    "  \n",
    "  layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden_last, num_labels], stddev=0.1))\n",
    "  layer5_biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "\n",
    "  # Model.\n",
    "  def model(data, use_dropout=False):\n",
    "    conv = tf.nn.conv2d(data, layerconv1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.elu(conv + layerconv1_biases)\n",
    "    pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    conv = tf.nn.conv2d(pool, layerconv2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.elu(conv + layerconv2_biases)\n",
    "    #pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "\n",
    "    conv = tf.nn.conv2d(hidden, layerconv3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.elu(conv + layerconv3_biases)\n",
    "    pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    # norm1\n",
    "    # norm1 = tf.nn.lrn(pool, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "    \n",
    "    conv = tf.nn.conv2d(pool, layerconv4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.elu(conv + layerconv4_biases)\n",
    "    pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    # norm1 = tf.nn.lrn(pool, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "\n",
    "    \n",
    "    conv = tf.nn.conv2d(pool, layerconv5_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.elu(conv + layerconv5_biases)\n",
    "    pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    # norm1 = tf.nn.lrn(pool, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "    \n",
    "    shape = pool.get_shape().as_list()\n",
    "    print shape\n",
    "    reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.elu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    \n",
    "    if use_dropout:\n",
    "        hidden = tf.nn.dropout(hidden, 0.75)\n",
    "    \n",
    "    nn_hidden_layer = tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "    hidden = tf.nn.elu(nn_hidden_layer)\n",
    "    \n",
    "    if use_dropout:\n",
    "        hidden = tf.nn.dropout(hidden, 0.75)\n",
    "    \n",
    "    \n",
    "    return tf.matmul(hidden, layer5_weights) + layer5_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset, True)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.1, global_step, 3000, 0.86, staircase=True)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "\n",
    "\n",
    "num_steps = 95001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print \"Initialized\"\n",
    "  for step in xrange(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print \"Minibatch loss at step\", step, \":\", l\n",
    "      print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "      print \"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels)\n",
    "      print time.ctime()\n",
    "  print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
